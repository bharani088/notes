> ref: ***Pattern Recognition and Machine Learning***

# 绪论

训练数据的样本包含输入向量以及对应的目标向量的应用叫做有监督学习(supervised learning)问题。数字识别就是这个问题的一个例子，它的目标是给每个输入向量分配到有限数量离散标签中的一个，被称为分类(classification)问题。如果要求的输出由一个或者多个连续 变量组成，那么这个任务被称为回归(regression)。


在其他的模式识别问题中，训练数据由一组输入向量x组成，没有任何对应的目标值。 在这样的无监督学习(unsupervised learning)问题中，目标可能是发现数据中相似样本的 分组，这被称为聚类(clustering)，或者决定输入空间中数据的分布，这被称为密度估计 (density estimation)，或者把数据从高维空间投影到二维或者三维空间，为了数据可视化 (visualization)。

本章也将介绍将自始至终在本书中使用的三个重要 工具:概率论、决策论、信息论。虽然这些东西听起来让人感觉害怕，但是实际上它们非常直 观。并且，在实际应用中，如果想让机器学习技术发挥最大作用的话，清楚地理解它们是必须 的。

## 概率论

在模式识别领域的一个关键概念是不确定性的概念。它可以由测量的误差引起，也可以由数 据集的有限大小引起。概率论提供了一个合理的框架，用来对不确定性进行量化和计算。概率 论还构成了模式识别的一个中心基础。当与决策论(1.5节讨论)结合，概率论让我们能够根据 所有能得到的信息做出最优的预测，即使信息可能是不完全的或者是含糊的。

The Rules of Probability

加和规则(sum rule)
```p(X) = ∑_Y(p(X,Y))```

乘积规则(product rule)
```p(X,Y) = p(Y|X)p(X)```

>这里p(X,Y)是联合概率，可以表述为“X且Y的概率”。类似地，p(Y |X)是条件概率，可以表 述为“给定X的条件下Y 的概率”，p(X)是边缘概率，可以简单地表述为“X的概率”。这两个简单 的规则组成了我们在全书中使用的全部概率推导的基础。

根据乘积规则，以及对称性p(X, Y ) = p(Y, X), 我们立即得到
```p(Y|X) = p(X|Y)p(Y) / p(X) ```
这被称为**贝叶斯定理(Bayes' theorem)**，在模式识别和机器学习领域扮演者中心角色。

使用加和规则，贝叶斯定理中的分母可以用出现在分子中的项表示:
```p(X) = ∑_Y(p(X|Y)p(Y))```

最后，如果两个变量的联合分布可以分解成两个边缘分布的乘积，即p(X, Y ) = p(X )p(Y )， 那么我们说X和Y **相互独立(independent)**. 根据乘积规则，我们可以得到p(Y | X) = p(Y )， 因此对于给定X的条件下的Y 的条件分布实际上独立于X的值。

### 概率密度

如果一个实值变量x的概率 落在区间 (x, x + δx) 的概率由 p(x)δx (δx → 0) 给出, 则 p(x) 叫做x的概率密度(probability density).
```p(x ∈ (a, b)) = ∫a~b p(x) dx.```

概率的加和规则和乘积规则以及贝叶斯规则，同样可以应用于概率密度函数的情形，也可以应用于离散变量与连续变量相结合的情形。
```
p(x) = ∫p(x,y)dy
p(x,y) = p(y|x)p(x) 
```

### 期望和协方差

在概率分布p(x)下，函数f(x)的平均值被称为f(x)的**期望(expectation)**，记作`E[f]`
```
# for discret distributions
E[f] = ∑_x(p(x)f(x))

# for continuous variables
E[f] = ∫p(x)f(x)dx

# if we are given a finite number N of points
E[f] ≃ 1/N * ∑_1~N f(x_n)

# We can also consider a conditional expectation with respect to a conditional
distribution
Ex[f|y] = ∑_x p(x|y)f(x)

# f(x)的方差(variance)被定义为
var[f] = E[(f(x)−E[f(x)])^2]
var[f] = E[f(x)^2] − E[f(x)]^2
# 变量x自身的方差
var[x] = E[x^2] − E[x]^2

# 对于两个随机变量x和y，协方差(covariance)被定义为
# 它表示在多大程度上x和y会共同变化。
# 如果x和y相互独立，那么它们的协方差为0。 
cov[x, y] 
= Ex,y [{x − E[x]} {y − E[y]}]
= Ex,y [xy] − E[x]E[y]

# 在两个随机向量x和y的情形下，协方差是一个矩阵
cov[x, y] 
= Ex,y {x − E[x]}{yT − E[yT]}
= Ex,y[xyT] − E[x]E[yT].

# 向量x各个分量之间的协方差可简记为
cov[x] ≡ cov[x, x]
```

### 贝叶斯概率

贝叶斯定理(Bayes’ theorem)的形式为
```p(w|D) = p(D|w)p(w) / p(D)```
它让我们能够通过后验概率p(w|D)，在观测到D之后估计w的不确定性

贝叶斯定理右侧的量`p(D|w)`由观测数据集D来估计，可以被看成参数向量w的函数，被称为似然函数(likelihood function), 它表达了在不同的参数向量w下，观测数据出现的可能性的大小

给定似然函数的定义，我们可以用自然语言表述贝叶斯定理:
```
posterior = likelihood × prior / evidence
posterior ∝ likelihood × prior
```



### 高斯分布(Gaussian distribution)
```
# 定义
N(x|μ,σ^2) = 1/(2πσ^2)^0.5 exp{-1/2σ^2 * (x-μ)^2}
# 归一化
∫-∞_+∞ N(x|μ,σ^2)dx = 1
# x均值
E[x] = ∫-∞_+∞ N(x|μ,σ^2)xdx = μ
# x方差
var[x] = E[x^2] - E[x]^2 = σ^2

# D维向量x的高斯分布
N(x|μ,Σ)= 1/[(2π)^(D/2) * |Σ|^0.5] exp{ −1/2(x−μ)T Σ^−1 (x−μ) }

# 两个独立事件的联合概率可以由各个事件的边缘 概率的乘积得到
# 由于我们的数据集x是独立同分布的，因此给定μ和σ2，我们可以给出数据集的概率(似然函数)
p(x|μ,σ^2) = ∏_n=1~N N(x_n|μ,σ^2)

# 通过最大化对数似然函数的对数lnp(x|μ,σ^2)来确定高斯分布中未知的参数μ和σ^2
# 关于μ，最大化函数可以得到最大似然解，这是样本均值
μ_ML = 1/N Σ_n=1~N x_n
# 类似地，关于σ^2最大化函数得到方差的最大似然解，这是关于样本均值μML的样本方差
σ^2_ML = 1/N Σ_n=1~N (x_n-μ_ML)^2
# 最大似然方法系统化地低估了分布的方差。这是一种叫做偏移(bias)的现象的例子，与多项式曲线拟合问题中遇到的过拟合问题相关
# 最大似然估计的平均值将会得到正确的均值，但是将会低估方差
# 实际应用中，只要 N 的值不太小，那么偏移的现象不是个大问题
# 最大似然的偏移问题是我们在多项式曲线拟合问题中遇到的过拟合问题的核心
```


### 重新考察曲线拟合问题？？

给定x的值，对应的t值服从高斯分布，分布的均值为y(x, w)，精度由参数β给出，它与方差的关系为β^−1 = σ^2
```
p(t | x,w,β) = N(t | y(x,w,β^−1))
```
用训练数据{x, t}，通过最大似然方法，来决定未知参数w和β的值。

似然函数为
```
p(x|μ,β) = ∏_n=1~N N(t_n|y(x_n,w), β^-1)
```

### 贝叶斯曲线拟合？？

我们知道训练数据X和T，我们想估计预测分布p(t | x, X, T)。
在一个纯粹的贝叶斯方法中，我们应该自始至终地应用概率的加和规则和乘积规则。
预测概率可以写成：

p(t | x,X,T) = ∫ p(t | x,w)p(w | X,T) dw

这里p(t | x,w)由公式`p(t | x,w,β) = N(t | y(x,w,β^−1))`给出,
p(w | x, t)是参数的后验分布，可以通过对公式`p(w | x,t,α,β) ∝ p(t | x,w,β)p(w | α)`归一化得到

预测分布由高斯的形式给出:
p(t | x,X,T) = N (t | m(x),s^2(x))


## 模型选择

我们已经看到，在最大似然方法中，由于过拟合现象，模型在训练集上的表现并不能很好地表示模型对于未知数据的预测能力。
如果数据量很大，那么模型选择很简单。在许多实际应用中，训练数据和测试数据都是很有限的。
解决这种困境的一种方法是使用交叉验证(cross validation)

## 维度灾难(curse of dimensionality)

在多项式曲线拟合的例子中，我们只有一个输入变量x。但是对于模式识别的实际应用来 说，我们不得不处理由许多输入变量组成的高维空间。

一种简单的方式是把输入空间划分成小的单元格。这种朴素的观点有很多问题。如果我们把空间的 区域分割成一个个的单元格，那么这些单元格的数量会随着空间的维数以指数的形式增大。当 单元格的数量指数增大时，为了保证单元格不为空，我们就不得不需要指数量级的训练数据。

如果我们有D个输入变量，随着D的增加，多项式的独立的系数的数量(并非所有的系数都独立，因为变量x之间的互换对称性)的 增长速度正比于D^3。对于一个M阶多项式，系数数量的增长速度类似于D^M。

在高维空间中，一个球体的大部分体积都聚集在表面附近的薄球壳上！ 


## 决策论

简要介绍一下决策论的关键思想。
首先非形式化地考虑一下概率论如何在做决策时起作用。

当我们得到一个新病人的X光片x时，我们的目标是判断这个X光片属于两类中的哪一类。
我们感兴趣的是在给定这个图像的前提下，两个类的概率，即`p(Ck | x)`

使用贝叶斯定理，
```
p(Ck | x) = p(x | Ck)p(Ck) / p(x)
```

出现在贝叶斯定理中的任意一个量都可以从联合分布p(x, Ck)中得到，要么通过积分的方式，要么通过关于某个合适的变量求条件概率。我们现在把p(Ck)称为类Ck的先验概率， 把p(Ck | x)称为对应的后验概率。
因此p(C1)表示在我们拍X光之前，一个人患癌症的概率。类似地，p(C1 | x)表示使用X光中包含的信息通过贝叶斯定理修改之后的对应的后验概率。

目标是最小化把x分到错误类别中的可能性，我们要选择有最大后验概率的 类别。

### 最小化错误分类率
```
p(mistake) = p(x ∈ R1, C2) + p(x ∈ R2, C1)
= ∫p(x, C2) dx + ∫p(x, C1) dx
```
为了最小化p(mistake)，如果p(x, C1) > p(x, C2)，那么我们就把x分到类别C1中，p(x,Ck) = p(Ck | x)p(x)。
如果我们把每个x分配到后验概率p(Ck | x)最大的类别中，那么我们分类错误的概率就会最小。

对于更一般的K 类的情形，
```
p(correct) = ∑k=1~K p(x ∈ Rk, Ck) 
=∑k=1~K ∫Rk p(x, Ck)dx
```
当区域Rk 的选择使得每个x都被分到使p(x, Ck )最大的类别中时，上式取得最大值。
再一次使用 乘积规则p(x, Ck) = p(Ck | x)p(x)，并且注意到因子p(x)对于所有项都相同，
我们可以看到每 个x都应该被分到有着最大后验概率p(Ck | x)的类别中。

### 最小化期望损失
假设对于新的x的值，真实 的类别为Ck，我们把x分类为Cj(其中j可能与k相等，也可能不相等)。这样做的结果是，我们 会造成某种程度的损失，记作Lkj ，它可以看成损失矩阵(loss matrix)的第k, j个元素。

|    | 癌症 | 正常 
|--- |---  |---
|癌症 | 0   | 100
|正常 | 1   | 0

例如， 在癌症的例子中，我们可能有图所示的损失矩阵。这个特别的损失矩阵表明，如果我们做出 了正确的决策，那么不会造成损失。如果健康人被诊断为患有癌症，那么损失为1。但是如果一 个患有癌症的病人被诊断为健康，那么损失为1000。

最小化期望损失的决策规则是对于 每个新的x，把它分到能使下式取得最小值的第j 类:
```
∑_k Lkj p(Ck | x)
```
一旦我们知道了类的后验概率p(Ck | x)之后，这件事就很容易做了。

### 拒绝选项
如果输入x使得两个后验概率分布中较大的那个概率分布小于或等于某个阈值θ，那么x会被拒绝识别。
注意，令θ = 1会使所有的样本都被拒绝，而如果有K个类别，那么令θ<1/K将会确保没有样本被拒绝。因此被拒绝的样本比例由θ的值控制。

### 推断和决策
三种解决decision problem的不同方式

a) **生成式模型(generative model)**:

首先对于每个类别Ck，独立地确定类条件密度p(x | Ck)。这是一个推断问题。然后，推 断先验类概率p(Ck )。
之后，使用贝叶斯定理`p(Ck | x) = p(x | Ck)p(Ck) / p(x)`求出后验类概率p(Ck | x)。
分母可以用分子中出现的项表示`p(x) = ∑_k p(x | Ck)p(Ck)`，等价地，我们可以直接对联合概率分布p(x, Ck )建模，然后归一化，得到后验概率。
显式地或者隐式地对输入以及输 出进行建模的方法被称为生成式模型(generative model)，因为通过取样，可以用来人工生成 出输入空间的数据点。

>需要求解的东西最多，因为它涉及到寻找 在x和Ck 上的联合概率分布。对于许多应用，x的维度很高，这会导致我们需要大量的训练数据 才能在合理的精度下确定类条件概率密度p(x | Ck)。注意，先验概率p(Ck )经常能够根据训练数据集里的 每个类别的数据点所占的比例简单地估计出来。
>但是，方法(a)的一个优点是，它能够通过公式`p(x) = ∑_k p(x | Ck)p(Ck)`求出数据的边缘概率密度p(x)。这对于检测模型中具有低概率的新数据点很有用，然而，如果我们只想进行分类的决策，那么这种方法会浪费计算资源。并且，实际上我们只 是想求出后验概率p(Ck | x)(可以直接通过方法(b)求出)，但是为了求出它，这种方法需要大 量的数据来寻找联合概率p(x, Ck )。事实上，类条件密度可能包含很多对于后验概率几乎没有影 响的结构，如图1.27所示。

b) **判别式模型(discriminant model)**: 

首先解决确定后验类密度p(Ck | x)这一推断问题，接下来使用决策论来对新的输入x进行 分类。这种直接对后验概率建模的方法被称为判别式模型(discriminative models)。

c) **判别函数(discriminant function)**:

找到一个函数f(x)，被称为判别函数。这个函数把每个输入x直接映射为类别标签。例 如，在二分类问题中，f(·)可能是一个二元的数值，f = 0表示类别C1，f = 1表示类别C2。这种 情况下，概率不起作用。

>使用方法(c)，我们不在能够接触到后验概率p(Ck | x)。有很多强烈的理由需要计算后 验概率，即使我们接下来要使用后验概率来进行决策



## 信息论

当我们观察到这个变量的一个具体值的时候，我们 接收到了多少信息呢?信息量可以被看成在学习x的值的时候的“惊讶程度”。
```
h(x, y) = h(x) + h(y)
p(x, y) = p(x)p(y)
```
h(x)一定与p(x)的对数有关
```
h(x) = − log2 p(x)
```
负号确保了信息一定是正数或者是零。注意，低概率事件x对应于高的信息量。对数的底 的选择是任意的。现在我们将遵循信息论的普遍传统，使用2作为对数的底。在这种情形下，正 如我们稍后会看到的那样，h(x)的单位是比特(bit, binary digit)

随机变量x的熵(entropy)
```
H[x] = − ∑_x p(x) log_2 p(x)
```
非均匀分布比均匀分布的熵要小。

现在，让我们考虑如何把变量状态的类别传递给接收者。与之前 一样，我们可以使用一个3比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特 点，使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件。

熵和最短编码长度的这种关系是一种普遍的情形。

对于定义在多元连续变量(联合起来记作向量x)上的概率密度，微分熵为
```
H[x] = − ∫ p(x)lnp(x) dx
```

给定x的情况下，y的条件熵
```
H[y | x] = − ∫∫ p(y,x)lnp(y|x) dydx
```
条件熵 H[Y|X]，先需定义当给定 X=x 时，Y 的熵。即:
```
H(Y | X = x) = - ∫ p(y|x)lnp(y|x) dy
```

使用乘积规则，很容易看出，条件熵满足下面的关系
```
H[x, y] = H[y | x] + H[x]
```




# 概率分布

Parametric method: assume a specific functional form for the distribution.

Nonparametric method: form of distribution typically depends on the size of the data set. Such
models still contain parameters, but control the model complexity rather than the form of the distribution.

共轭先验(conjugate prior)有着很重要的作用。
在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。

Conjugate prior的意义:方便进行Bayesian inference，甚至是sequential Bayesian inference sequential Bayesian inference:
得到一个 observation 后，可以算出 posterior;由于选取的是 共轭先验，所以 posterior 和原来的 prior 形式一样，可以把该 posterior 当作新的 prior，用于 下一个 observation，如此迭代下去。对于 stream of data 的情况，这种方式可以实现 real-time
learning。

| Distribution | Conjugate Prior
| ------------ | ---------------
| Bernoulli    | Beta distribution
| Multinomial  | Dirichlet distribution
| Gaussian , Given variance, mean unknown | Gaussian distribution
| Gaussian, Given mean, variance unknown  | Gamma distribution
| Gaussian, both mean and variance unknown| Gaussian-Gamma distribution

## 二元变量

> [似然函数](https://zh.wikipedia.org/zh-hans/似然函数)
> [二项分布](https://zh.wikipedia.org/wiki/二項分佈)

```
# 考虑一个二元随机变量x ∈ {0, 1}
p(x = 1 | μ) = μ
p(x = 0 | μ) = 1 − μ
# 这被叫做伯努利分布(Bernoulli distribution)
Bern(x | μ) = μ^x * (1 − μ)^(1−x)
# 期望
E[x] = ∑i=0~1 xi * fX(x) = 0 + μ = μ
# 方差
var[x] = ∑i=0~1 (xi-E[x])^2 * fX(x) = (0-μ)^2 * (1-μ) + (1-μ)^2 * μ = μ(1-μ)
```

假设我们有一个x的观测值的数据集D = {x1,...,xN}
```
# 构造关于μ的似然函数如下
p(D | μ) = ∏_n=1~N p(xn | μ) = ∏_n=1~N μ^xn * (1−μ)^(1−xn)
# 我们可以通过最大化(对数)似然函数来估计μ的值
lnp(D | μ) = ∑_n=1~N { xnlnμ + (1-xn)ln(1-μ) }
# 令ln p(D | μ)关于μ的导数等于零，我们就得到了最大似然的估计值，这也被成为样本均值(sample mean)
μML = 1/N * ∑_n=1~N xn
# x=1的次数记为m，则
μML = m/N

# 二项分布(binomial distribution)
# 是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p
# 当n = 1时，二项分布就是伯努利分布

# 求解给定数据集规模N的条件下，x = 1的观测出现的数量m的概率分布。这被称为二项分布
# 在N次抛掷中，我们必须把所有获 得m个正面朝上的方式都加起来
Bin(m|N,μ) = (N;m) μ^m * (1−μ)^(N−m)
# 其中(N;m) = N! / (N-m)!m!
# 对于独立的事 件，加和的均值等于均值的加和，加和的方差等于方差的加和。由于m = x1 + . . . + xN ，并且 对于每次观察，均值和方差都分别由公式(2.3)和公式(2.4)给出，因此
# 期望
E[m] = Nμ
# 方差
var[m] = Nμ(1 − μ)
```

### Beta分布
> [通俗理解beta分布](https://www.zhihu.com/question/30269898)

如果一个数据集里有m次观测为x = 1，有l次观测为x = 0，那么从先验概率到后 验概率，a的值变大了m，b的值变大了l。这让我们可以简单地把先验概率中的超参数a和b分别 看成x = 1和x = 0的有效观测数(effective number of observation)。注意，a和b不一定是整数。另外，如果我们接下来观测到更多的数据，那么后验概率分布可以扮演先验概率的角色。
```
p(x = 1 | D) = (m+a) / (m+a + l+b)
```
在数据集无限大的极限情况下，m, l → ∞，此时公式的结果变成了最大似然的结果。对于有限规模的数据集，μ的 后验均值总是位于先验均值和公式`μML = 1/N * ∑_n=1~N xn`给出的μ的最大似然估计之间。

当观测的数量增加时，后验分布的图像变得更尖了。如果a → ∞或 者b → ∞，那么方差就趋于零。
贝叶斯学习的一个共有的属性:随着我们观测到越来越多的数据，后验概率表示的不确定性将会持续下降。


## 多项式变量

考虑一个有N个独立观测值x1, . . . , xN 的数据集D。对应的似然函数的形式为
```
p(D | μ) = ∏_n=1~N ∏_k=1~K μk^xnk 
= ∏_k=1~K μk^(∑xnk)
= ∏_k=1~K μk^mk
```
最大似然解
`μkML = mk / N`
它是N次观测中，xk = 1的观测所占的比例

多项式分布(multinomial distribution)
```
Mult(m1,m2,...,mK | μ,N) = (N;m1...mK) ∏_k=1~K μk^mk

mk = ∑_n xnk 它表示观测到xk = 1的次数
(N;m1...mK) = N! / m1!m2!...mK!
```

“1-of-K”表示法：变量被表示成一个K维向 量x，向量中的一个元素xk等于1，剩余的元素等于0。

it models the probability of counts for rolling a k-sided die n times. 
When n is 1 and k is 2, the multinomial distribution is the Bernoulli distribution. When k is 2 and number of trials are more than 1, it is the binomial distribution.

### 狄利克雷分布(Dirichlet distribution)？？
狄利克雷分布是一组连续多变量概率分布，是多变量普遍化的Β分布。
```
Dir(μ | α) = Γ(α0)/Γ(α1)...Γ(αk) * ∏_k=1~K μk^(αk-1)
= Dir(μ | α+m) = Γ(α0+N)/Γ(α1+m1)...Γ(αk+mk) * ∏_k=1~K μk^(αk+mk-1)
```
与二项分布的先验概率为Beta分布相同，我们可以把狄利克雷分布的参数αk看成xk = 1的有效观测数


## 高斯分布
> [正态分布](https://zh.wikipedia.org/wiki/正态分布)

正态分布是如何发现的，在《数理统计简史》有详细的介绍，当时已经有很多人包括拉普拉斯在找随机误差的分布形式，都没有找到，高斯是出于一个假设找到的，也就是随机误差分布的最大似然解是算数平均值，只有正态分布这个函数满足这个要求。

对于单个实值变量，使熵最大的是高斯分布。这个性质同样适用于多变量高斯。另一种情况是，多个随机变量之和也会产生高斯分布。中心极限定理（由拉普拉斯提出）告诉我们，温和的一组随机变量的和的概率分布随着项的增加，趋向于高斯分布（Walker, 1969）。为了阐述这个，考虑 NN 个区间[0, 1]上的均匀分布的随机变量 x1,...,xNx1,...,xN ，确定它们的均值 (x1+...+xn)/N(x1+...+xn)/N 的分布。在实际应用中，随着N的增加，分布会很快的收敛于高斯分布。二元随机变量 x 在 N 次观测中出现 m 次的二项分布将会在 N→∞ 时趋向于高斯分布

对于D维向量x，多元高斯分布
> [多维高斯分布是如何由一维发展而来的？](https://www.zhihu.com/question/36339816)

高斯分布的另一个局限性是它本质上是单峰的(即只有一个最大值)，因此不能够很好地近 似多峰分布。因此高斯分布一方面相当灵活，因为它有很多参数。另一方面，它又有很大的局 限性，因为它不能够近似很多概率分布。我们稍后会看到，引入潜在变量(latent variable)， 也被称为隐藏变量(hidden variable)或者未观察变量(unobserved variable)，会让这两个问题 都得到解决。

### 条件高斯分布
多元高斯分布的一个重要性质是，如果两组变量是联合高斯分布，那么以一组变量为条件， 另一组变量同样是高斯分布。类似地，任何一个变量的边缘分布也是高斯分布。

### 边缘高斯分布
我们已经看到，如果联合分布p(xa, xb)是高斯分布，那么条件概率分布`p(xa | xb)`也是高斯 分布。现在我们要讨论边缘概率分布
```
p(xa)=  p(xa,xb)dxb
```
正如我们即将看到的那样，这也是一个高斯分布。

### 高斯变量的贝叶斯定理
给定x的一个边缘高斯分布，以及在给定x的条件下y的条件高斯分布，形式为 
```
p(x) = N(x | μ,Λ^−1)
p(y | x) = N (y | Ax + b, L^−1)
```
y的边缘分布以及给定y的条件下x的条件分布为
```
p(y) = N (y | Aμ + b, L^−1 + A Λ^−1 A^T ) 
p(x | y) = N (x | Σ{A^T L(y − b) + Λμ}, Σ)
Σ = (Λ + A^T L A)^−1
```

### 高斯分布的最大似然估计
```
μML = 1/N * Σ_n=1~N x
ΣML = 1/N * Σ_n=1~N (xn − μML)(xn − μML)^T
E[μML] = μ
E[ΣML] = (N-1)/N * Σ
```

### 顺序估计
顺序的方法允许每次处理一个数据点，然后丢弃这个点。这对于在线应用很重要。并且当数据集相当大以至于一次处理所有数据点不可行的情况下，顺序方法也很重要。

考虑均值的最大似然估计结果μML。当它依赖于第N次观察时，将被 记作μ(N ) 。如果我们想分析最后一个数据点xN 的贡献，我们有
```
μML(N) = 1/N * Σ_n=1~N xn
= 1/N * xN + 1/N * Σ_n=1~N-1 xn
= 1/N * xN + N-1/N * μML(N−1)
= μML(N−1) + 1/N * (xN − μML(N−1))
```
在观察到N−1个数据点后，我们已经把μ估计 为μML(N−1) 。我们现在观察到了数据点xN ，这样我们就得到了一个修正的估计μML(N) ，这个估计的获得方式为:把旧的估计沿着“错误信号”(xN − μML(N−1))方向移动一个微小的量，这个量正比于1/N。注意，随着N的增加，后续数据点的贡献也会逐渐变小。

### 高斯分布的贝叶斯推断
高斯分布的精度的共轭先验是Gamma分布

### 学生t分布
t分布的一个重要性质:鲁棒性(robustness)，意思是对于数据集里的几 个离群点outlier的出现，t分布不会像高斯分布那样敏感。

### 周期变量
有些情况下，对于连续变量，使用高斯分布建模并不合适。一个重要的情况是周 期变量，这在实际应用中经常出现。

我们可能试图这样处理周期变量:选择一个方向作为原点，然后应用传统的概率分布(例如 高斯分布)。但是，这种方法的结果将会强烈依赖于原点的选择。

为了找到均值的一个不变的度量，我们注意到观测可以被看做单位圆上的点，因此可以被描述为 一个二维单位向量x1, . . . , xN ，其中∥xn∥ = 1且n = 1, . . . , N。这个定义将会保证均值的位置与极坐标原点的选择
无关。x ̄ 通常位于单位圆的内部。

这被称为von Mises分布，或者环形正态分布(circular normal)。

### 混合高斯模型
通过将更基本的概率分布(例如高斯分布)进行线性组合的这样的叠加方法，可以被形式化 为概率模型，被称为混合模型(mixture distributions)。高斯分布的线性组合可以给出相当复杂的概率密度形 式。通过使用足够多的高斯分布，并且调节它们的均值和方差以及线性组合的系数，几乎所有 的连续概率密度都能够以任意的精度近似。

考虑K 个高斯概率密度的叠加，形式为
```
p(x) = Σ_k=1~K πk N (x | μk, Σk)
```
这被称为混合高斯(mixture of Gaussians)

## 指数族分布

参数为η的变量x的指数族分布定义为具有下面形式的概率分布的集合
```
p(x | η) = h(x)g(η) exp{ηT u(x)}
```
其中x可能是标量或者向量，可能是离散的或者是连续的。这里η被称为概率分布的自然参数 (natural parameters)，u(x)是x的某个函数。函数g(η)可以被看成系数，它确保了概率分布是
归一化的，因此满足
``` 
g(η) ∫ h(x) exp{ηT u(x)} dx = 1
```

### 最大似然与充分统计量
### 共轭先验
### 无信息先验


## 非参数化方法
密度估计的直方图方法:
标准的直方图简单地把x划分成不同的宽度为∆i的箱 子，然后对落在第i个箱子中的x的观测数量ni进行计数。为了把这种计数转换成归一化的概率 密度，我们简单地把观测数量除以观测的总数N，再除以箱子的宽度∆i，得到每个箱子的概率的值
```
pi = ni / N∆i
```
容易看出`∫p(x)dx = 1`，这个概率密度在每个箱子的宽度内是常数，并且通常箱子的宽度选成相同的，即∆i = ∆。

我们看到，当∆非常小的时候(最上方的图)，最终的概率密 度模型有很多尖刺，有很多结构没有出现在生成数据的概率分布中。相反，如果∆过大(最下 方的图)，那么最终的概率模型会过于平滑，结果无法描述绿色曲线的双峰性质。当∆取一个 中等大小的值时(中间的图)，可以得到最好的结果。

### 核密度估计
考虑包含x的某个小区域R。**这个区域的概率质量P为**
```
P = ∫_R p(x) dx
```
假设我们收集了服从p(x)分布的N次观测。由于**每个数据点都有一个落在区域R中的概率P**，
因此**位于区域R内部的数据点的总数K**将服从二项分布
```
Bin(K |N,P)= N!/K!(N-K)! P^K (1−P)^(N−K)
```
落在区域内部的数据点的平均比例为`E[K/N] = P`，
以此为均值的概率分布的方差为`var[K/N] = P(1−P)/N`
```
# 对于大的N值
K ≃ NP
# 假定区域R足够小，使得在这个区域内的概率密度p(x)大致为常数，V 是区域R的体积
P ≃ p(x)V
# 结合以上
p(x) = K / NV
```
我们可以固定K然后从数据中确定V的值，这就是**K近邻方法**。
我们还可以固定V然后从数据中确定K，这就是**核方法**。

### 近邻方法
p(x) ≃ K / NV*

# 回归的线性模型
## 线性基函数模型
### 最大似然与最小平方
### 最小平方的几何描述
### 顺序学习
### 正则化最小平方
### 多个输出

## 偏置-方差分解

## 贝叶斯线性回归
### 参数分布

## 贝叶斯模型比较
## 证据近似
## 固定基函数的局限性

# 分类的线性模型