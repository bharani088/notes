>ref: ***统计学习方法***

> - Classification: kNN, DecisionTree, NaiveBayes, LogisticRegression, SVM, AdaBoost
> - Regression: LinearRegression, TreeRegression (CART)
> - EM, HMM, CRF

# 概论

统计学习分为，监督学习（supervised learning），非监督学习，半监督学习和强化学习（reinforcement learning），其中以监督学习最为常见和重要

统计学习的分类

- **生成模型(generative model)**

    - 由数据**学习联合概率分布P(X，Y)，然后求出条件概率分布P(Y|X)**作为预测模型，即生成模型
    - 典型的如，朴素贝叶斯和隐马尔可夫模型 
    - 优点是可以得到联合概率分布，收敛速度更快，当存在隐变量时，仍可以使用。

- **判别模型(discriminative model)**

    - 由数据**直接学习决策函数f(X)或条件概率分布P(Y|X)**作为预测模型，即判别模型
    - 典型的如，KNN，感知机，决策树，逻辑回归，支持向量等 
    - 优点是学习准确率比较高，便于对数据进行抽象，可以简化学习问题。

判别模型与生成模型的最重要的不同是，训练时的目标不同，判别模型主要优化条件概率分布，使得x,y更加对应，在分类中就是更可分。而生成模型主要是优化训练数据的联合分布概率。而同时，生成模型可以通过贝叶斯得到判别模型，但判别模型无法得到生成模型。

统计学习还可以根据输入输出的不同类型，分为

- **回归**问题 
  
  - 输入输出都是连续变量，就是回归问题，等价于函数拟合
  - 最常用的损失函数是平方损失函数，可由最小二乘法(least squares)求解

- **分类**问题 

  - 输出变量是有限个离散值时，就是分类问题 
  - 学习出的分类模型或分类决策函数称为分类器（classifier） 

- **标注**问题 

  - 输入是一个观测序列，而输出是一个标记序列 
  - 典型的应用，词性标注(POS tagging)，输入词序列，输出是（词，词性）的标记序列 
  - 常用的是隐马尔可夫模型、条件随机场

## 统计学习的过程 
1. 获取训练数据集合 
2. 确定假设空间，即所有可能的模型的集合 
3. 确定模型选择的准则（什么是最优模型的标准），即学习的策略 
4. 实现求解最优模型的算法（如何获取最优模型），即学习的算法 
5. 通过算法中假设空间中，找到最优模型 
6. 使用该模型对新数据进行预测和分析

## 统计学习三要素: 
- 模型
    - 非概率模型，**决策函数(decision function)**
    ```F = {f | Y=f(X)}```
    - 概率模型，**条件概率分布(conditional probability distribution)**
    ```F = {P | P(Y|X)}```
- 策略
    - 用**损失函数(loss function)**L(Y，f(X))，度量一次预测的结果好坏
        - **0-1**损失函数 `L(Y，f(X)) = {1，Y!=f(X) | 0，Y=f(X)}`
        - **平方(quadratic)**损失函数 `L(Y，f(X)) = (Y-f(X))^2`
        - **绝对(absolute)**损失函数 `L(Y，f(X)) = |Y-f(X)|`
        - **对数(logarithmic)**损失函数 `L(Y，P(Y|X)) = -logP(Y|X)`
    - **经验风险(empirical risk)**最小化与**结构风险(structural risk)**最小化
        - 经验风险：平均损失
        - 结构风险：为了防止过拟合，加入**正则化项(regularizer)**
- 算法
    - 用什么样的方法来求解最优模型，这样统计学习模型就归结为**最优化问题(optimization problem)**

## 模型评估与模型选择
- 训练误差(training error) 测试误差(test error)
- 过拟合(over-fitting)
    - bias variance tradeoff

## 正则化与交叉验证：两种常用模型选择方法
- **正则化(regularizarion)**
    - 正则化项：可以是模型参数向量的**范数**
        - **L0**：向量中**非0的元素的个数**（希望W的大部分元素都是0，是稀疏解；NP难问题）
        - **L1**：向量中各元素**绝对值之和**（L1范数是L0范数的**最优凸近似**）
        - **L2**：向量中各元素的平方和然后求平方根，即向量的**模**（希望W的每个元素都很小，都接近于0，与L1范数不同的是不会让它等于0）
    - 作用是选择经验风险和模型复杂度同时较小的模型
    - 符合奥卡姆剃刀原理：“如无必要，勿增实体”，即“简单有效原理”
    - 贝叶斯估计的角度：正则化项对应于模型的先验概率，简单模型有较大先验
- **交叉验证(cross validation)**
    - 简单交叉验证：随机地将数据70%作为训练集，30%作为测试集，然后选出测试误差最小的模型 
    - S折交叉验证：将数据随机分成S份，将S-1份作为训练集，剩下的作为测试集，对于训练集和测试集有S种选择，所以选出S次评测的平均误差最小的模型 
    - 留一交叉验证：S-fold的特例，用于数据缺乏的情况，S=N

## 泛化误差
- 泛化误差：模型对未知数据预测的误差（期望风险）
- 泛化误差上界：
    - 是样本容量的函数，样本容量增加时，泛化误差上界趋于0；
    - 是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。


# 感知机（perceptron）

## 感知机模型

**二类分类的线性分类模型**。是神经网络和支持向量机的基础。

```
f(x) = sign(wx+b) = {+1， if w'*x+b>=0 || -1， else}
```

几何解释为， 线性方程，wx+b=0，对应于特征空间中的一个**分离超平面**（separating hyperplane），其中w为超平面的法向量，b是超平面的截距。该平面将数据点分为正，负两类。

## 感知机学习策略

如何选取损失函数很关键

这里一个自然的选择是，用误分点的总数作为损失函数，但问题是这个损失函数和w，b没关系，不易优化。 
所以这里**选择误分点到超平面的总距离作为损失函数**，这样损失函数对于w，b是连续可导的，这样就可以使用梯度下降来找到最优解。

空间中任意一点x0到超平面S的距离为```1/||w|| * |w*x0+b|```

由于符号函数的不连续性，如果采用标准的均方误差，所得误差函数必然是不连续的，因而基于梯度的学习算法也就不能被使用。
**对于误分类的(xi，yi)，`-yi(w*xi+b) > 0`，用来替换`|w'*x+b|`**，
误分点到超平面的总距离为```-1/||w|| * Σ(yi(w*xi+b))```，不考虑常数1/||w||则

**感知机sign(wx+b)的损失函数：**

```
L(w,b) = - Σ(yi(w*xi+b))
```

感知机学习算法是**误分类驱动**的，L(w,b)对于w和b连续可导的，

```
▽w L(w,b) = -Σyixi
▽b L(w,b) = -Σyi
```

这里使用**梯度下降法（gradient descent）**来找到最优解。

![](http://ww4.sinaimg.cn/large/6cbb8645jw1eormbaytduj20jw0880ty.jpg)

> [梯度下降法的三种形式BGD、SGD以及MBGD](https://www.cnblogs.com/maybe2030/p/5089753.html)

## 感知机学习算法的**对偶形式**

对偶形式的基本想法：将w和b表示为实例xi和标记yi的线性组合的形式，通过求解其系数而求得w和b.

最终求解到的参数分别表示为

![](http://ww1.sinaimg.cn/large/6cbb8645jw1eosqfeu4yqj2038039jr8.jpg)

这里，`αi = ni*η`，对误分类点i逐步修改w、b修改了ni次，ni越大，意味着它与分离超平面的距离越近，越难以正确分类（很可能是support vector）

![](http://ww1.sinaimg.cn/large/6cbb8645jw1eosqijr2q3j20k30cpdhg.jpg)


# k近邻（k-NN， k-Nearest Neighbors）

k近邻算法是一种用于分类和回归的非参数统计方法。
在这两种情况下，输入包含特征空间中的k个最接近的训练样本。

- 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的**“多数表决”**确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若k = 1，则该对象的类别直接由最近的一个节点赋予。
- 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的**平均值**。

最近邻居法采用向量空间模型来分类。相同类别的案例，彼此的相似度高，而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。

无论是分类还是回归，衡量**邻居的权重**都非常有用，使较近邻居的权重比较远邻居的权重大。例如，一种常见的加权方案是给每个邻居权重赋值为1/ d，其中d是到邻居的距离。（克服“多数表决”分类在类别分布偏斜时出现缺陷：出现频率较多的样本将会主导测试点的预测结果）

对n维实数向量空间Rn，经常用Lp距离或曼哈顿Minkowski距离。

Lp距离定义如下：

![Lp距离](http://ww1.sinaimg.cn/large/6cbb8645jw1eoxb1nfk9wj207m01xdfp.jpg)

用图表示如下：

![](http://ww1.sinaimg.cn/large/6cbb8645jw1eoxb5rzj1ej208q09a3yu.jpg)

当p=2时，称为欧氏距离；当p=1时，称为曼哈顿距离；当p=∞，它是各个坐标距离的最大值

一般情况下，将**欧氏距离**作为距离度量，但是这是只适用于连续变量。在文本分类这种离散变量情况下，另一个度量——重叠度量（或**海明距离**）可以用来作为度量。

## k值的选择

k较小，容易被噪声影响，发生过拟合。

k较大，较远的训练实例也会对预测起作用，容易发生错误。

k一般会选取比较小的值，通常采用交叉验证来选取最优的k值

## 分类决策规则

多数表决规则的解释：使用0-1损失函数衡量，如果涵盖Nk(x)的区域的类别是cj，那么误分类率是：

![](http://ww3.sinaimg.cn/large/6cbb8645jw1eoxbxbn6bfj209o01wa9z.jpg)

Nk是近邻集合，要使左边最小，右边必须最大，所以**多数表决等价于经验风险最小化**。

## kd树

> ref: <https://zhuanlan.zhihu.com/p/26029567>

## k近邻算法 —— 线性扫描，蛮算，复杂度O(DN)

![](http://images.cnitblog.com/blog/312753/201403/181820483967575.png)

##  k近邻算法 —— kd树，复杂度O(DlogN)

> ref: [【数学】kd 树算法之详细篇](https://zhuanlan.zhihu.com/p/23966698)

# 朴素贝叶斯

> ref: [贝叶斯推断定理简介](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html)
> ref: [朴素贝叶斯分类器](https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)
> ref: [朴素贝叶斯分类器 - b站](https://www.bilibili.com/video/av7719936/)

贝叶斯推断是一种统计推断方法，其中贝叶斯定理用于在更多证据或信息可知时更新假设的概率。

朴素贝叶斯法是**基于贝叶斯定理与特征条件独立假设**的分类方法，对于给定输入x，求出**后验概率最大的**输出y。

```
p(H|E) = P(E|H)*P(H) / P(E)
p(H|E) = P(E|H)/P(E) * P(H)
posterior = likelihood/evidence * prior
```

## 朴素贝叶斯概率模型

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f2c8595ffd1c98706f679d2586ccb73c95336d71)
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/679e25db34f602d562e503af6d772125f78ab31e)

实际中，我们只关心分式中的分子部分，因为分母不依赖于C而且特征Fi的值是给定的，于是分母可以认为是一个常数。

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0be67f84d7213f91f3ea647923462aab194ab86d)

现在“朴素”的条件独立假设开始发挥作用:假设每个特征Fi对于其他特征Fj， j≠i是条件独立的。

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/14ae825a0763ef510201f3f3bc22ae21728d3b6d)
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7751cf399cb6004329f4a6547a9ef54afbe8d8ab)
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7e2c830eac90468e5839385df574ae31d4fc1dbc)

其中Z(证据因子)是一个只依赖与F1...Fn等的缩放因子，当特征变量的值已知时是一个常数。

朴素贝叶斯分类器包括了这种模型和相应的决策规则: **最大后验概率(MAP)**决策准则（等价于期望风险最小化）。相应的分类器：
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1f164389c02ead6cc42c72ab5821032961af8f01)

## 参数估计

### 极大似然估计

> ref: [似然函数(Likelihood Function)](https://csruiliu.github.io/blog/2016/05/31/likelihood_function/)
>
> 通过已知的参数去预测事件的结果，这个过程就是计算概率，通过观测事件的结果去推测参数，这个过程就是似然估计。
>
> 总之，似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大。所以，对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。

所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的**极大似然估计**

![](http://images0.cnblogs.com/blog/673322/201507/241123100681947.png)

![](http://images0.cnblogs.com/blog/673322/201507/241123114286432.png)

### 贝叶斯估计

和极大似然估计差不多，贝叶斯估计只是在极大似然估计上添加了一个**平滑**

条件概率的贝叶斯估计:
![](http://images0.cnblogs.com/blog/673322/201507/241123122408316.png)

先验概率的贝叶斯估计:
![](http://images0.cnblogs.com/blog/673322/201507/241123133819101.png)

式中λ>=0，等价于在随机变量各个取值的频数上加λ。当λ=0就是极大似然估计。**常取λ=1，这时称为拉普拉斯平滑**



# 决策树 (Decision Tree)

决策树可以看成一系列if-then的规则

## 决策树学习 

决策树学习的本质是从训练数据集上归纳出**一组分类规则**， 
但与训练集不相矛盾的决策树可能有多个，也可能一个都没有， 
我们的目标是找到一个和训练集**矛盾较小**，且具有很好的**泛化能力**的决策树（注意不要求没有矛盾，要防止overfit）

决策树学习的**损失函数**通常是**正则化的极大似然函数**，但是基于损失函数找到全局最优决策树是**NP完全问题**， 
所以实际使用的算法通常采用启发式的方法，即**局部最优**。决策树学习的算法通常是一个**递归地选择最忧特征，并根据该特征对训练数据进行分割**。
具体做法就是，每次选择feature时，都挑选择当前条件下最优的那个feature作为划分规则，即局部最优的feature。

**决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。**

决策树学习通常分为3个步骤：**特征选择**，**决策树生成**和**决策树的修剪**

## 特征选择

选择特征的标准是找出**局部最优的特征**，即**以这个特征进行分类后，数据集是否更近有序**（不同分类的数据被尽量分开），还是仍然很混乱？ 

这里要使用**熵（entropy）**和**信息增益**
> ref: [熵 (信息论)](http://zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29)
> ref: [了解信息增益和决策树](http://www.cnblogs.com/wentingtu/archive/2012/03/24/2416235.html)

熵：表示随机变量**不确定性**的度量，反应的是事物的**无序程度**，或者说**随机变量分布的均匀程度**。

**信息增益**的算法：

![](http://img.blog.csdn.net/20170103223027986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGFveWFucWk4OTMy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**信息增益比**: 单纯的信息增益只是个相对值，因为这依赖于H(D)的大小，所以定义信息增益比作为更加客观的度量 

![](http://img.blog.csdn.net/20160720150024857)

## ID3算法（信息增益），C4.5算法（信息增益比）
![](http://img.blog.csdn.net/20150430124204271?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveW91MTMxNDUyMG1l/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

C4.5只是把其中的信息增益换成信息增益比

## 剪枝

前面提到决策树容易产生过拟合现象。为此我们需要对其剪枝处理。剪枝是从已生成的树上裁掉一些子树或子结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。

决策树剪枝往往是通过极小化决策树整体的损失函数（代价函数）来实现的。设树T有叶结点个数为`|T|`，t是树T的叶结点，该叶结点有N_t个样本点，其中k类样本点数有个N_tk，k=1，2，⋯，K。为叶结点t上的经验熵。a≥0为参数，则决策树学习的**损失函数**（Loss function）可以定义为： 

![](http://img.blog.csdn.net/20161116163331580)

![](http://img.blog.csdn.net/20161116163403581)

C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。|T|为模型复杂度α≥0控制两者之间的影响。较大的α促使选择较简单的模型(树)，较小的α促使选择较复杂的模型树。α=0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

剪枝就是α当确定时，选择损失函数最新的模型。当α确定，子树越大，往往训练数据拟合越好。但是模型的复杂度就越高；相反，子树越小，往往与训练数据的拟合就越低，但是往往与训练数据拟合不好，损失函数正好表示两者的平衡。

**损失函数的极小化等价于正则化的极大似然估计。**

![](http://i.imgur.com/8ojVtsK.png)
![](http://i.imgur.com/J9fv2TK.png)

## CART算法

分类与回归树(classification and regression tree，CART)是一种广泛的决策树学习方法，其中它既可以做分类也可以做回归，同时CART与ID3和C4.5所不同的一点是他是个**二叉树**，即每次划分都是是与不是。

CART的生成就是**递归的构建二叉决策树**的过程，其中对**回归树**用**平方误差最小化的准则**，对**分类树**用**基尼指数(Gini index)最小化的准则**。

### 回归树的生成

一个回归树对应于将输入空间D划分为M个单元R1，R2，...Rm，并且在每个单元上的输出值为在此单元上所有的数据的平均值，即如果测试的要本落在了Ri这个空间中，则其输出的值为：
```
cm = ave(yi|xi∈Rm)
```

主要的问题是我们如何进行空间的划分，下面给出**最小二乘回归树生成算法**：

![](http://img.blog.csdn.net/20170104211909871?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGFveWFucWk4OTMy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

基尼指数： 

分类问题中，假设有K个类，样本点属于第K类概率p_k，则概率分布的基尼指数定义为： 

![](http://img.blog.csdn.net/20161116164713727)

在CART的算法中，生成的是二叉树，因此k=2，所以基尼指数变为： 
```
Gini(p) = 2p(1−p)
```

Gini(D)基尼指数表示集合D的不确定性，基尼指数Gini(D，A)表示A=a分割后集合D的不确定性，基尼指数越大，样本集合的不确定性就越大。这点和熵类似。 

D根据特征A是否取某一个可能值a而分为D1和D2两部分：

![](http://ww1.sinaimg.cn/large/6cbb8645jw1ep3m8ns7u9j209f011glg.jpg)

则在特征A的条件下，D的基尼指数是：

![](http://ww4.sinaimg.cn/large/6cbb8645jw1ep3m9dfgdij208y01ngli.jpg)

**CART生成算法**：

设节点的当前数据集为D，对D中每一个特征A，对其每个值a根据D中样本点是否满足A==a分为两部分，计算基尼指数。选择基尼指数选择最小的特征及其对应的切分点作为最优特征和最优切分点，生成两个子节点，将对应的两个分区分配过去，然后对两个子节点递归调用这个过程，直到满足停止条件（结点中的样本个数小于预定阀值，或样本的基尼指数小于预定阀值）。

### CART剪枝

![](https://raw.githubusercontent.com/SamanthaChen/GitPicbed/master/ML/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/cartJianzhi.png)

# 逻辑回归与最大熵模型 (logistic regression & maximum entropy model)

## 逻辑斯谛分布

设X是连续随机变量，X服从逻辑斯谛分布是指X服从如下分布函数和密度函数，其中，μ为位置参数，λ> 0为形状参数
![](http://ww3.sinaimg.cn/large/6cbb8645gw1ewjeeb7m58j20ny05e0t4.jpg)
![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewjejmez18j20jw08dq3g.jpg)

### 二项逻辑斯蒂回归模型

是一种由条件概率分布`P(Y|X)`表示的模型:

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewjj2lcsrqj20c605uq3a.jpg)

定义事件的**几率(odds)**:发生概率与不发生概率的比值: `p/(1-p)`

在逻辑斯蒂回归模型中，**输出Y=1的对数几率是输入x的线性函数**:

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewjjh6a0uij20810290sn.jpg)

### 模型参数估计

应用**极大似然估计法**估计模型参数

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewjk8k757sj20db01cmx3.jpg)

**似然函数**为

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewjkbzax6ij208802bwed.jpg)

**对数似然函数**为

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewjkdwbm4mj20ee06i0t6.jpg)

对L(w)求极大值，得到w的估计值。这样，问题就变成了**以对数似然函数为目标函数的最优化问题**。

LR学习中通常采用**梯度下降法**及**拟牛顿法**。

### 多项逻辑斯蒂回归

![](http://img.blog.csdn.net/20150717111424598)

## 最大熵模型

> 考虑怎么把最大熵模型推导出logistic回归模型。
> 
> [如何理解最大熵模型里面的特征？ - Semiring的回答 - 知乎](https://www.zhihu.com/question/24094554/answer/108271031)


> 熵：表示随机变量**不确定性**的度量，反应的是事物的**无序程度**，或者说**随机变量分布的均匀程度**。

最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。

最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。

首先，模型必需满足联合分布P（X，Y）的经验分布和边缘分布P（X）的**经验分布**，即

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewm09bdrlzj207j02t3yf.jpg)

其中，v(X=x,Y=y)表示T中（x,y）出现频数，v(X=x)表示x出现的频数，N表示样本容量。

用特征函数f(x,y)定义x,y之间某一事实，其定义是：

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewm0ch201gj207b01rdfp.jpg)

这是一个二值函数，接受x和y作为参数，输出0或1。

特征函数f(x,y)关于经验分布
![](http://ww4.sinaimg.cn/large/6cbb8645gw1ewm0e000tnj201n00va9t.jpg)
的期望用
![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewm0epwsqbj201f00s0sh.jpg)
表示。

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewm0fag3qcj205d019q2r.jpg)

特征函数f(x,y)关于模型P（Y|X）与经验分布
![](http://ww3.sinaimg.cn/large/6cbb8645gw1ewm0ikmge2j201800u0sh.jpg)
的期望用
![](http://ww3.sinaimg.cn/large/6cbb8645gw1ewm0j1snprj201g00s3y9.jpg)
表示。

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewm0jo34r9j206g01gjr8.jpg)

**如果模型能获取训练数据中的规律的话，那么这两个期望应该是相等的**，也即

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewm0nsd837j203m00z3ya.jpg)

或

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewm0n3dvvsj208i01jdfq.jpg )

这个式子就是**最大熵模型学习的约束条件**，如果有n个特征函数，那么就有n个这样的约束条件。

### 最大熵模型的定义

假设满足约束条件的模型集合为

![](http://ww4.sinaimg.cn/large/6cbb8645gw1ewm0pstv05j208q012mx0.jpg)

定义在条件概率分布P（Y|X）上的**条件熵**为

![](http://ww1.sinaimg.cn/large/6cbb8645gw1ewm0qowhmrj207b017t8k.jpg)

则模型集合C中条件熵H(P)最大的模型称为最大熵模型。

### 最大熵模型的学习

> 详细推导过程参考原书 or [最大熵模型的学习](http://www.hankcs.com/ml/the-logistic-regression-and-the-maximum-entropy-model.html#h3-12)

![](http://img.blog.csdn.net/20161011170501580?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

这里，使用拉格朗日对偶性，将约束最优化问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题（由于拉格朗日函数L(P,w)是P的凸函数，所以可以做这种等价）。

具体求解和推导参考原书。

### 极大似然估计

（略，证明**对偶函数的极大化等价于最大熵模型的极大似然估计**）

所以最大熵模型的学习问题就转换为具体求解对数似然函数或对偶函数极大化的问题。

可以将最大熵模型写成更一般的形式。

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewmknt0w9pj207w01tt8l.jpg)

其中

![](http://ww3.sinaimg.cn/large/6cbb8645gw1ewm2wcfaxuj206r01nt8k.jpg)

对数似然函数为：

![](http://ww2.sinaimg.cn/large/6cbb8645gw1ewmle00t4tj20a101idfr.jpg)

最大熵模型和逻辑斯蒂模型有类似的形式，它们又称为**对数线性模型(log linear model)**。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或者**正则化的极大似然估计**。

## 模型学习的最优化算法

### 改进的迭代尺度法

（略）

### 拟牛顿法

（附录B）

# 支持向量机

> ref: [支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/v_july_v/article/details/7624837)
> ref: [支持向量机 - 码农场](http://www.hankcs.com/ml/support-vector-machine.html)
> ref: [空间任意点到超平面的距离计算](http://www.cnblogs.com/chyl411/p/5584801.html)

## 线性可分支持向量机与硬间隔最大化

??采用拉格朗日对偶问题进行求解

线性可分支持向量机学习算法——最大间隔法
![](http://ww3.sinaimg.cn/large/6cbb8645gw1ewsf9syt2bj20hf09lwfl.jpg)

??线性可分支持向量机的优化问题求解

## (线性不可分)线性支持向量机与软间隔最大化

线性可分支持向量机学习算法
![](http://img.blog.csdn.net/20170330115127807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvanl0MTEyOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

## 非线性支持向量机与核函数


# 提升方法

提升方法就是从弱学习算法出发，反复学习得到一系列弱分类器，然后组合这些弱分类器构成一个强大的分类器。

算法本身是改变数据分布实现的，它根据每次训练集之中的每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值（增大被误分的样本的权值，保证下层分类器能够更充分地考虑这些被误分的样本）。将修改权值的新数据送给下层分类器进行训练，然后将每次训练得到的分类器融合起来，作为最后的决策分类器。

## Adaboost

AdaBoost算法特点是通过迭代每次学习的一个基本分类器，每次迭代中，提高那些被前一轮分类器错误分类数据的权值，降低那些被正确分类的数据的权值，最后AdaBoost将基本分类器线性组合为强分类器，其中给分类误差率小基本分类器较大的权值，给分类误差大的基本分类器以小的权值

AdaBoost算法的基本思想:

1.多轮训练，多个分类器

2.每轮训练增加错误分类样本的权值，降低正确分类样本的权值

3.降低错误率高的分类器的权值，增加正确率高的分类器的权值

![](http://img.blog.csdn.net/20160713172734090?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![](http://img.blog.csdn.net/20160713172756316?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![](http://img.blog.csdn.net/20160713173109560?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![](http://img.blog.csdn.net/20160713173152920?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

## 前向分步算法与Adaboost：

![](http://upload-images.jianshu.io/upload_images/4155986-dc5f554b3962dcdc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由前向分步算法可以推导出AdaBoost，用定理叙述这一关系。

定理：AdaBoost算法是前向分歩加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。

## 提升树算法

以决策树为基函数的提升方法成为提升树（boosting tree）

提升树是以分类树和回归树为基本分类器的提升方法，提升树被认为是统计学习中比较有效的学习方法

对分类问题，决策树是二叉分类树，对回归问题决策树是二叉回归树

# EM算法
TODO

# 隐马尔可夫模型
TODO

# 条件随机场
TODO
